#+TITLE: Wielowymiarowa analiza danych
 #+author: Hubert Baran 164141, Patryk Gronkiewicz 164157
 #+email: 164141@stud.prz.edu, 164157@stud.prz.edu.pl
 #+language: pl
 #+latex_class: report

* Użyte oprogramowanie
** ETL --- [[https://kafka.apache.org][Apache Kafka]][fn:kafka]

   Apache Kafka to platforma open-source do rozproszonego przetwarzania danych w postaci strumieniowej.

   Na początku była rozwijana przez LinkedIn, następnie otrzymała wsparcie od Apache Software Foundation.
   Napisana jest w językach Scala i Java.

   W uproszczeniu, pełni rolę pośrednika pomiędzy instancjami wysyłającymi dane (sender, producer)
   i odbierającymi je (receiver, consumer). Kafka przetwarza jednostki danych zwane zdarzeniami lub
   wiadomościami (events, messages). Przykładowym zdarzeniem może być np. transakcja finansowa, zmiana
   współrzędnych geograficznych, odczyt sensora w urządzeniu IoT itp. Zdarzenia (events) są przyporządkowane
   do kategorii zwanych topics. Konsumenci (consumers) mogą otrzymywać dane, gdy zasubskrybują jakiś topic- wówczas otrzymują
   dane w postaci strumienia (stream).

   Do Kafki podłączamy 4 typy elementów:
   + Producers - aplikacje udostępniające strumienie
   + Consumers - aplikacje subskrybujące i odbierające strumienie
   + Stream Processors - aplikacje pobierające strumienie i produkujące kolejne strumienie (transformujące strumienie)
   + Connectors - procesy/aplikacje łączące topiki Kafki z jakimiś aplikacjami, systemami bazodanowymi itp.
   Dla każdego z tych typów mamy osobne API Kafki.
   
   W naszym przypadku Producentem jest aplikacja Prometheus, a konsumentem baza danych Cockroach. W celu ładowania danych z Prometheusa użyto odpowiedniego [[https://github.com/Telefonica/prometheus-kafka-adapter][adaptera]][fn:adapter] stworzonego przez firmę Telefónica.

** Data warehouse --- [[https://www.cockroachlabs.com/][CockroachDB]][fn:cockroach]

CockroachDB to rozproszona baza danych, której core jest dostępny za darmo.

Wysokopoziomowo to relacyjna baza danych, wykorzystuje SQL.
Niskopoziomowo wykorzystuje mechanizm typu klucz-wartość, o wysokim stopniu spójności.

Baza została zaprojektowana do wysokiej i niskokosztowej skalowalności przy zachowaniu dużej
odporności na awarie.

CockroachDB wspiera protokół wymiany danych bazy PostgreSQL, co oznacza, że jest kompatybilna z
tą bazą danych (i narzędziami opartymi o nią).

** Zarządzanie usługami --- [[https://ww.wdocker.com][Docker]][fn:docker]

Docker to platforma Open Source umożliwiająca tworzenie kontenerów (containers)
i uruchamianie w nich aplikacji.

Kontenery są formą wirtualizacji. Są to aplikacje, które izolują jakiś kod (program) wraz
z potrzebnymi mu bibliotekami od reszty środowiska (systemu). W kontenerach jesteśmy w stanie
uruchamiać aplikacje napisane pod system Linux w systemie Windows.

Kontenery różnią się od maszyn wirtualnych tym, że wykorzystują jądro systemu operacyjnego gospodarza -
nie instalujemy kolejnego systemu operacyjnego "w całości", jedynie potrzebne komponenty.
Są więc znacznie lżejsze.

Aby uruchomić kontener w systemie Windows, musimy wpierw podjąć następujące kroki:
+ w panelu Funkcje systemu Windows uruchamiamy potrzebne funkcje:.
(Platforma funkcji Hypervisor systemu Windows,Platforma maszyn wirtualnych, Podsystem Windows dla
systemu Linux. Może być konieczne ponowne uruchomienie komputera.)
+ zainstalowanie [[https://docs.microsoft.com/en-us/windows/wsl/install][WSL 2]]

W projekcie wykorzystujemy mechanizm docker compose. Za jego pomocą z użyciem jednego pliku
instrukcji dla Dockera jesteśmy w stanie automatycznie wygenerować kontenery dla wszystkich
potrzebnych nam serwisów - Docker sam pobierze obrazy (images) kontenerów z repozytorium,
a następnie je uruchomi.

Nasz plik z instrukcjami docker compose: docker-compose.yml. Widzimy tam, że tworzymy osobny
kontener dla każdej z aplikacji: Zookeeper, Kafka, adapter do Kafki, Grafana, CockroachDB.
W pliku określamy nazwę obrazu kontenera i parametry takie jak woluminy, porty przez które
możliwa będzie komunikacja.

** Tworzenie wykresów --- [[https://grafana.com/][Grafana]][fn:grafana]

Grafana to system open source przeznaczony do monitorowania i analizy zbiorów danych
(zwykle dużych, poszerzających się w czasie, często pochodzących z wielu źródeł).
Jest to aplikacja webowa (korzystamy z niej z użyciem przeglądarki internetowej).
Umożliwia m. in.:
+ tworzenie zapytań
+ interaktywną wizualizację
+ alertowanie

W Grafanie tworzymy dashboardy - raporty w formie pulpitów prezentujące wizualne analizy
(głównie wizualizacje). Podstawowym "klockiem", z którego budujemy dashboard, jest panel -
dla każdego panelu tworzymy zapytanie, definiujemy typ wizualizacji itd.

Dostępnych jest wiele typów wizualizacji - np. wykres liniowy, wykres słupkowy,
wskaźnik (gauge), mapa cieplna (heatmap) i inne. Można też wyświetlać wyniki obliczeń w
formie liczb i tekst.

W Grafanie możemy dodawać wiele źródeł danych. Wspieranych jest większość
popularnie używanych DBMS - w tym relacyjne, NoSQL, bazy do szeregów czasowych.

** Źródło danych --- [[https://prometheus.io][Prometheus]][fn:prometheus]

[fn:kafka][[https://kafka.apache.org]]
[fn:confluent]https://github.com/confluentinc/confluent-kafka-python
[fn:cockroach]https://www.cockroachlabs.com/
[fn:docker]https://www.docker.com
[fn:grafana]https://grafana.com/
[fn:prometheus]https://prometheus.io
[fn:adapter]https://github.com/Telefonica/prometheus-kafka-adapter
* Uruchomienie projektu
** Instalacja zależności
Cały projekt można podzielić na trzy kluczowe elementy:
1. Usługi zewnętrzne;
2. Transformacje danych w ETL;
3. Analiza danych przetworzonych.

Pierwszy element jako jedyną zależność posiada Dockera. Dwie następne natomiast opierają się o Pythona w wersji $\geq 3.8$. Całą instrukcję instalacji Dockera można znaleźć pod [[https://docs.docker.com/get-docker/][tym adresem]][fn:docker-installation]. Po zainstalowaniu oraz wejściu do folderu głównego wystarczy uruchomić polecenie
#+begin_src shell-script :eval never
docker compose up
#+end_src
W przypadku braku podkomendy ~compose~ należy pobrać ~docker-compose~ z repozytorium Pythona przez komendę oraz uruchomić usługi
#+begin_src shell-script :eval never
pip install -U docker-compose
docker-compose up
#+end_src
W celu wyłączenia usług należy na klawiaturze wcisnąć =Ctrl= + =C=.

[fn:docker-installation] https://docs.docker.com/get-docker/

Pozostałe części projektu można przygotować do uruchomienia uruchomienie następujących komend (instrukcje dla Linuxa, dla Windowsa zmienia się jedynie pierwsza komenda).
#+begin_src shell-script :eval never
source .venv/bin/activate.sh
pip install -r requirements.txt
python FOLDER/main.py
#+end_src
Przy czym =FOLDER= należy zastąpić odpowiednią paczką, którą chcemy uruchomić w danym momencie.
** Uruchomienie
* Implementacja
** Generowanie danych
** ETL
*** Eksport danych z Prometheusa
*** Transformacje danych w Pythonie
*** Ładowanie danych do CockroachDB
** Analiza danych
*** Machine Learning
